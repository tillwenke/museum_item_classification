{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/till/projects/uni/data science/museum_item_classification/setup_general.py:92: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_intermediate_ready = pd.read_csv('./data/general/combined_intermediate_ready.csv', index_col='id', dtype={'type': str})\n"
     ]
    }
   ],
   "source": [
    "from setup_general import *\n",
    "from setup_embedding import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation - extract text from ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_text(item):\n",
    "    return ' '.join(item[text_features]).strip()\n",
    "\n",
    "data = prep.copy()\n",
    "\n",
    "text_features = ['name', 'commentary', 'text', 'legend', 'initial_info', 'additional_text']\n",
    "data[text_features] = data[text_features].fillna('')\n",
    "data['text_features'] = data.apply(lambda item: collect_text(item),axis=1)\n",
    "\n",
    "data.to_csv('data/prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prep.copy()\n",
    "with_damages = combined_data_fully_translated.copy()\n",
    "\n",
    "data.text_features = data.text_features.replace(float('nan'), ' ',)\n",
    "with_damages.damages = with_damages.damages.replace(float('nan'), ' ',)\n",
    "\n",
    "data.text_features = data.text_features + ' ' + with_damages.damages\n",
    "\n",
    "data.to_csv('data/prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prep.copy()\n",
    "text = data[['text_features','type','source']]\n",
    "text.to_csv('data/text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text.copy()\n",
    "data.text_features = data.text_features.apply(lambda x: x.strip())\n",
    "data = data[data.text_features != '']\n",
    "data.to_csv('data/text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = text.copy()\n",
    "train = df[df.source == 'train']\n",
    "test = df[df.source == 'test']\n",
    "train.to_csv('data/train_text.csv', index=True)\n",
    "test.to_csv('data/test_text.csv', index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract in Estonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36267/1302915245.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text.text_features = text.text_features.apply(lambda x: x.strip())\n"
     ]
    }
   ],
   "source": [
    "def collect_text(item):\n",
    "    return ' '.join(item[text_features]).strip()\n",
    "\n",
    "data = combined_data.copy()\n",
    "\n",
    "text_features = ['name', 'commentary', 'text', 'legend', 'initial_info', 'additional_text', 'damages']\n",
    "data[text_features] = data[text_features].fillna('')\n",
    "data['text_features'] = data.apply(lambda item: collect_text(item),axis=1)\n",
    "\n",
    "text = data[['text_features','type','source']]\n",
    "\n",
    "text.text_features = text.text_features.apply(lambda x: x.strip())\n",
    "text = text[text.text_features != '']\n",
    "\n",
    "train = text[text.source == 'train'].drop('source', axis=1)\n",
    "test = text[text.source == 'test'].drop('source', axis=1)\n",
    "train.to_csv('data/text/train_text_est.csv', index=True)\n",
    "test.to_csv('data/text/test_text_est.csv', index=True)\n",
    "text.to_csv('data/text/text_est.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt3 text embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '***'\n",
    "count = 0\n",
    "def get_embedding(text, model=\"text-similarity-davinci-001\"):\n",
    "    global count\n",
    "    count += 1\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    try:\n",
    "        result = openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "    except:\n",
    "        print(count)\n",
    "        time.sleep(60)\n",
    "        result = openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "    return result\n",
    " \n",
    "text['curie_similarity'] = text.text_features.apply(lambda x: get_embedding(x, model='text-similarity-curie-001'))\n",
    "text.to_csv('data/text_embeddings/curie.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = curie.copy()\n",
    "# remove unneeded cols\n",
    "\n",
    "data.drop(columns=['text_features'], inplace=True)\n",
    "data.set_index('id', inplace=True)\n",
    "split = pd.DataFrame(data.curie_similarity.tolist(), index=data.index)\n",
    "concat = pd.concat([data, split], axis=1)\n",
    "concat.drop(columns=['curie_similarity'], inplace=True)\n",
    "concat.to_csv('data/text_embeddings/curie.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train val split - embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_est_prepared.copy()\n",
    "val = val_est_prepared.copy()\n",
    "trainval_curie = curie[curie.source == 'train'].drop(columns=['source'])\n",
    "test_curie = curie[curie.source == 'test'].drop(columns=['source'])\n",
    "\n",
    "train_curie = pd.DataFrame.join(train[['element_count']], trainval_curie)\n",
    "train_curie.dropna(axis=0, inplace=True)\n",
    "train_curie.drop(columns=['element_count'], inplace=True)\n",
    "print(len(train_curie))\n",
    "val_curie = pd.DataFrame.join(val[['element_count']], trainval_curie)\n",
    "val_curie.dropna(axis=0, inplace=True)\n",
    "val_curie.drop(columns=['element_count'], inplace=True)\n",
    "print(len(val_curie))\n",
    "\n",
    "train_curie.to_csv('data/text_embeddings/train_curie.csv', index_label='id')\n",
    "val_curie.to_csv('data/text_embeddings/val_curie.csv', index_label='id')\n",
    "test_curie.to_csv('data/text_embeddings/test_curie.csv', index_label='id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words & train val split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/till/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7284\n",
      "3166\n"
     ]
    }
   ],
   "source": [
    "min_gram = 1\n",
    "max_gram = 2\n",
    "max_features = 4000\n",
    "\n",
    "dataset = text_est\n",
    "stop_words = stopwords_est\n",
    "\n",
    "CountVec = TfidfVectorizer(ngram_range=(min_gram,max_gram), stop_words=stop_words, max_features=max_features)\n",
    "# to use bigrams ngram_range=(2,2)\n",
    "Count_data = CountVec.fit_transform(dataset.text_features)\n",
    "\n",
    "#create dataframe\n",
    "bow=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
    "\n",
    "bow = bow.add_prefix('word_')\n",
    "bow.index = dataset.index\n",
    "bow = bow.join(dataset[['source', 'type']])\n",
    "\n",
    "train = train_est_prepared.copy()\n",
    "val = val_est_prepared.copy()\n",
    "trainval_bow = bow[bow.source == 'train'].drop(columns=['source'])\n",
    "test_bow = bow[bow.source == 'test'].drop(columns=['source'])\n",
    "\n",
    "train_bow = pd.DataFrame.join(train[['element_count']], trainval_bow)\n",
    "train_bow.dropna(axis=0, inplace=True)\n",
    "train_bow.drop(columns=['element_count'], inplace=True)\n",
    "print(len(train_bow))\n",
    "val_bow = pd.DataFrame.join(val[['element_count']], trainval_bow)\n",
    "val_bow.dropna(axis=0, inplace=True)\n",
    "val_bow.drop(columns=['element_count'], inplace=True)\n",
    "print(len(val_bow))\n",
    "\n",
    "train_bow.to_csv('data/text_embeddings/train_bow.csv', index_label='id')\n",
    "val_bow.to_csv('data/text_embeddings/val_bow.csv', index_label='id')\n",
    "test_bow.to_csv('data/text_embeddings/test_bow.csv', index_label='id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
