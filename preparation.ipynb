{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/till/projects/uni/data science/museum_item_classification/setup_general.py:95: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_intermediate_ready = pd.read_csv('./data/general/combined_intermediate_ready.csv', index_col='id', dtype={'type': str})\n"
     ]
    }
   ],
   "source": [
    "from setup_general import *\n",
    "lang = 'est'\n",
    "if lang == 'en':\n",
    "    data = combined_data_fully_translated.copy()\n",
    "if lang == 'est':\n",
    "    data = combined_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/typeless/data.csv', index_col='id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature specific engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## units - sizes -values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish unit translation/ unification &  values to float\n",
    "data['value'] = data['value'].apply(lambda x: float(x.replace(',', '.')) if type(x) == str else x)\n",
    "\n",
    "# unify units\n",
    "data['unit'] = data['unit'].replace('10 x 15 cm','100 x 150 mm')\n",
    "\n",
    "# mm to cm\n",
    "data['value'] = data.apply(lambda item: item['value'] / 10 if item['unit'] == 'mm' else item['value'], axis=1)\n",
    "data['unit'] = data['unit'].replace('mm','cm')\n",
    "data['value'] = pd.to_numeric(data['value'])\n",
    "\n",
    "# Combine parameter, unit & w/h values to value\n",
    "def get_squared(item):\n",
    "    if ' x ' in item:\n",
    "        return item + '²'\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "def extract_width_height_from_unit_to_value(item):\n",
    "    unit = item[0]\n",
    "    value = item[1]\n",
    "    if ' x ' in unit:\n",
    "        split = unit.split(' ')\n",
    "        x = split[0]\n",
    "        y = split[2]\n",
    "        real_unit = split[3]\n",
    "        real_value = [x,y]        \n",
    "        return [real_unit, real_value]\n",
    "\n",
    "    else:\n",
    "        return [unit, value]\n",
    "\n",
    "data['unit'] = data['unit'].replace(np.nan,'*')\n",
    "data['parameter'] = data['parameter'].replace(np.nan,'*')\n",
    "data['unit'] = data['unit'].apply(lambda x: get_squared(x))\n",
    "# execution order is important\n",
    "data['value'] = data.apply(lambda item: extract_width_height_from_unit_to_value(item[['unit','value']])[1], axis=1)\n",
    "data['unit'] = data.apply(lambda item: extract_width_height_from_unit_to_value(item[['unit','value']])[0], axis=1)\n",
    "data['parameter_and_unit'] = data['parameter'] + ' IN ' + data['unit']\n",
    "\n",
    "# parameter_and_units as single features with respective values\n",
    "# parameter_and_unit turned into one hot encoded features\n",
    "data= pd.get_dummies(data, columns=['parameter_and_unit'], prefix='', prefix_sep='')\n",
    "\n",
    "def extract_value(value, present):\n",
    "    if present == 1:\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "#  for all new \"parameter with unit\" columns put the value in the column where a 1 is - others are 0 and remain 0\n",
    "for column in data.columns:\n",
    "    if ' IN ' in column and '*' not in column:\n",
    "        data[column] = data.apply(lambda item: extract_value(item['value'], item[column]), axis=1)\n",
    "\n",
    "# make all size features numeric\n",
    "def extract_height_width(item):    \n",
    "    if item == 0:\n",
    "        return [0.0,0.0]\n",
    "    else:        \n",
    "        return [float(i) for i in item]\n",
    "        \n",
    "\n",
    "for column in data.columns:\n",
    "    # all the parameter with unit columns that contain arrays that are represeted as strings\n",
    "    if (' IN ' in column) and (data[column].dtype == object):\n",
    "        data[column + '_height'] = data.apply(lambda item: extract_height_width(item[column])[0], axis=1)\n",
    "        data[column + '_width'] = data.apply(lambda item: extract_height_width(item[column])[1], axis=1)\n",
    "        pd.to_numeric(data[column + '_height'])\n",
    "        pd.to_numeric(data[column + '_width'])\n",
    "        data = data.drop(column, axis=1)\n",
    "\n",
    "for column in data.columns:\n",
    "    if (' IN ' in column):\n",
    "        data[column] = data[column].replace(np.nan,0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## country_and_unit - technique (whitespace deletion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_to_nan(item):\n",
    "    if type(item) == str:\n",
    "        item = item.strip()\n",
    "        if item == '':\n",
    "            return np.nan\n",
    "        else:\n",
    "            return item\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "data['country_and_unit'] = data.apply(lambda x: empty_to_nan(x['country_and_unit']), axis=1)\n",
    "data['technique'] = data['technique'].apply(lambda x: x.strip() if (type(x) == str) else x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## country_unit - material - technique - location (splitting for features including multiple information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_unit\n",
    "def extract_city_country(item):      \n",
    "    if (type(item) == str):\n",
    "        item = item.strip()\n",
    "        # there are some empty (non-nan) values\n",
    "        if (item == ''):\n",
    "            return [float('nan'), float('nan')]\n",
    "    \n",
    "        item = re.sub(' +', ' ', item) # remove multiple spaces\n",
    "\n",
    "        if (' ' in item) and ('Eesti' in item):        \n",
    "            split = item.split(' ')\n",
    "            return [' '.join(split[1:]), split[0]]\n",
    "        else: \n",
    "            return [float('nan'), item]\n",
    "    else:\n",
    "        return [float('nan'), float('nan')]\n",
    "\n",
    "data['city_municipality'] = data.apply(lambda item: extract_city_country(item['country_and_unit'])[0], axis=1)\n",
    "data['country'] = data.apply(lambda item: extract_city_country(item['country_and_unit'])[1], axis=1)\n",
    "\n",
    "# material\n",
    "# to make the following work even for nan values\n",
    "data['material'] = data['material'].replace(np.nan, 'nan')\n",
    "# prepare single values to be distinguishable\n",
    "data['material'] = data['material'].apply(lambda x: x.split('>'))\n",
    "\n",
    "# https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "data = data.join(pd.DataFrame(mlb.fit_transform(data.pop('material')),\n",
    "                          columns='material_' + mlb.classes_,\n",
    "                          index=data.index))\n",
    "\n",
    "# technique\n",
    "# to make the following work even for nan values\n",
    "data['technique'] = data['technique'].replace(np.nan, 'nan')\n",
    "\n",
    "# prepare single values to be distinguishable\n",
    "data['technique'] = data['technique'].apply(lambda x: x.split('>'))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "data = data.join(pd.DataFrame(mlb.fit_transform(data.pop('technique')),\n",
    "                          columns='technique_' + mlb.classes_,\n",
    "                          index=data.index), rsuffix='')\n",
    "\n",
    "# location\n",
    "data['location_city'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('linn ' in x) else 0)\n",
    "data['location_building'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('hoone ' in x) else 0)\n",
    "data['location_street'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('tänav ' in x) else 0)\n",
    "data['location_country'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('riik ' in x) else 0)\n",
    "data['location_address'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('aadress ' in x) else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start - end (formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groups the year into its own column with the complete number, if NaN, then 0\n",
    "def year_Grouping(x):\n",
    "    xStr = str(x)\n",
    "    if xStr == 'nan':\n",
    "        return 0\n",
    "    if '.' in xStr:\n",
    "        xStr = xStr.split('.')\n",
    "        xStr = xStr[xStr.__len__()-1]\n",
    "        if xStr.__len__() == 2:\n",
    "            xStr = '19' + xStr\n",
    "        elif xStr == '':\n",
    "            return 0\n",
    "        return int(xStr)\n",
    "    else:\n",
    "        return int(xStr)\n",
    "\n",
    "# returns 1 if a month is given, 0 if not\n",
    "def month_Grouping(x):\n",
    "    xStr = str(x)\n",
    "    if '.' in xStr:\n",
    "        xStr = xStr.split('.')\n",
    "        if xStr[0].__len__() <= 2 and xStr[1].__len__() <= 2:\n",
    "            if xStr[1] == '':\n",
    "                return 0\n",
    "            elif xStr[1].__len__() == 1:\n",
    "                return 1\n",
    "            return 1\n",
    "        xStr = xStr[0]\n",
    "        if xStr == 'jaan':\n",
    "            return 1\n",
    "        elif xStr == 'veebr':\n",
    "            return 1\n",
    "        if xStr == 'märts':\n",
    "            return 1\n",
    "        elif xStr == 'apr':\n",
    "            return 1\n",
    "        elif xStr == 'mai':\n",
    "            return 1\n",
    "        elif xStr == 'juuni':\n",
    "            return 1\n",
    "        elif xStr == 'juuli':\n",
    "            return 1\n",
    "        elif xStr == 'aug':\n",
    "            return 1\n",
    "        elif xStr == 'sept':\n",
    "            return 1\n",
    "        elif xStr == 'okt':\n",
    "            return 1\n",
    "        elif xStr == 'nov':\n",
    "            return 1\n",
    "        elif xStr == 'dets':\n",
    "            return 1\n",
    "        elif xStr.__len__() == 1:\n",
    "            return 1\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#returns one if a day is given, 0 if not\n",
    "def day_Grouping(x):\n",
    "    xStr = str(x)\n",
    "    if '.' in xStr:\n",
    "        xStr = xStr.split('.')\n",
    "        if xStr[0].__len__() <= 2 and xStr[1].__len__() <= 2:\n",
    "            return 1\n",
    "        return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def extract_year_from_name(row):\n",
    "    name = row['name']\n",
    "    start = row['start']\n",
    "    if pd.isnull(start) and not pd.isnull(name):\n",
    "        match = re.search('\\d\\d\\d\\d', name)\n",
    "        if match:\n",
    "            start = match.group()\n",
    "    return start\n",
    "\n",
    "data['start'] = data[['name', 'start']].apply(extract_year_from_name, axis=1)    \n",
    "    \n",
    "#grouping applied to the dataframe\n",
    "data['startYear'] = data['start'].apply(year_Grouping)\n",
    "data['startMonth'] = data['start'].apply(month_Grouping)\n",
    "data['startDay'] = data['start'].apply(day_Grouping)\n",
    "        \n",
    "data['endYear'] = data['end'].apply(year_Grouping)\n",
    "data['endMonth'] = data['end'].apply(month_Grouping)\n",
    "data['endDay'] = data['end'].apply(day_Grouping)\n",
    "\n",
    "#if there is no start year, but an end year, then the start year is set to the end year\n",
    "for i in range(1,len(data)):\n",
    "    if data['startYear'].iloc[i] == 0 and data['startDay'].iloc[i] != 0:\n",
    "        data['startYear'].iloc[i] = data['endYear'].iloc[i]\n",
    "\n",
    "\n",
    "#original columns are dropped as they are no longer needed\n",
    "data.drop(['start', 'end'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## event_type (brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_brackets(x):\n",
    "    x = str(x)\n",
    "    x = x.strip('< >')\n",
    "    if '\\u200b' in x:\n",
    "        x = x.replace('\\u200b', '')\n",
    "    return x\n",
    "\n",
    "data['event_type'] = data['event_type'].apply(strip_brackets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## color (grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fa6bb3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'color'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/till/projects/uni/data science/museum_item_classification/preparation.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/till/projects/uni/data%20science/museum_item_classification/preparation.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/till/projects/uni/data%20science/museum_item_classification/preparation.ipynb#X22sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m#apply colour_grouping to the dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/till/projects/uni/data%20science/museum_item_classification/preparation.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mcolor\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mcolor\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mapply(colour_grouping)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'color'"
     ]
    }
   ],
   "source": [
    "#Grouping colours by their base colour - to avoid too many extra cloumns when hot encoding -> could always reverse this step\n",
    "#by using  something like data['color'] = combined_data_translated['color'] ?\n",
    "\n",
    "#The base colours: red, blue, green, grey, yellow, patterned, orange, brown, white, black , pink\n",
    "#The most common/distingtive stay unchanged\n",
    "\n",
    "\n",
    "def colour_grouping(x):\n",
    "    if x in ['madara red', 'dark red', 'purple red', 'Red']:\n",
    "        return 'red'\n",
    "    elif x in ['light blue', 'dark blue', 'purple blue', 'greenish blue', 'greyish blue']:\n",
    "        return 'blue'\n",
    "    elif x in ['light green', 'light olive green', 'grey-green', 'olive green', 'greyish-olive green', 'dark green']:\n",
    "        return 'green'\n",
    "    elif x in ['bluish grey', 'dark grey', 'pinkish gray']:\n",
    "        return 'grey'\n",
    "    elif x in ['pale yellow', 'light yellow', 'orange-yellow', 'brilliant yellow']:\n",
    "        return 'yellow'\n",
    "    elif x in ['brownish orange']:\n",
    "        return 'orange'\n",
    "    elif x in ['light brown', 'dark brown', 'greyish brown', 'reddish brown', 'olive brown', 'yellowish brown']:\n",
    "        return 'brown'\n",
    "    elif x in ['yellowish white', 'bluish white']:\n",
    "        return 'white'\n",
    "    elif x in ['brownish black']:\n",
    "        return 'black'\n",
    "    elif x in ['mauve pink']:\n",
    "        return 'pink'\n",
    "    elif x in ['<patterned>', 'striped', 'checkered']:\n",
    "        return 'patterned'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "#apply colour_grouping to the dataset\n",
    "data['color'] = data['color'].apply(colour_grouping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rescaling numerics values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric features\n",
    "numeric_features = [value for value in ['ks', 'musealia_seria_nr', 'musealia_queue_nr', 'collection_queue_nr', 'element_count'] if value in data.columns]\n",
    "# continous numeric features (nan -> 0)\n",
    "\n",
    "data[numeric_features] = data[numeric_features].replace(np.nan, 0)\n",
    "\n",
    "data[numeric_features] = MinMaxScaler().fit_transform(data[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./data/typeless/intermediate_ready_data.csv', index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNTIL HERE DATA INTERMEDIATELLY PREPED"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## technique - material - sizes (threshold previously encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 columns found that start with technique_\n",
      "111 columns found that start with material_\n"
     ]
    }
   ],
   "source": [
    "# best found combination (local optimum on 500 estimators)\n",
    "perc = 0.98\n",
    "threshold_sum = len(data) * perc\n",
    "min_freq = 7\n",
    "\n",
    "tech = prep_helpers.col_collection(data, 'technique_')\n",
    "mat = prep_helpers.col_collection(data, 'material_')\n",
    "size = data.columns[data.columns.str.contains('IN')]\n",
    "\n",
    "features = [tech,mat,size]\n",
    "\n",
    "for feat in features:\n",
    "    frequencies = {}\n",
    "    for col in feat:\n",
    "        frequencies[col] = data[col].sum()\n",
    "    frequencies = dict(sorted(frequencies.items(), key=lambda item: item[1], reverse=True))\n",
    "    instance_sum = 0\n",
    "    for col in frequencies:\n",
    "        frequency = frequencies[col]\n",
    "        #if instance_sum > threshold_sum or frequency < min_freq:\n",
    "        if frequency < min_freq:\n",
    "            data.drop(columns=[col], inplace=True)\n",
    "        instance_sum += frequency\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd4a3dba",
   "metadata": {},
   "source": [
    "## hot encoding & thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical columns\n",
    "# already encoded\n",
    "# material, technique, unit, size, value\n",
    "\n",
    "cols = ['musealia_additional_nr', 'collection_mark', 'musealia_mark', 'museum_abbr', 'before_Christ', 'is_original', 'class', 'parish', 'state',  'event_type', 'participants_role', 'parish', 'color', 'collection_additional_nr', 'damages', 'participant', 'location', 'name', 'commentary', 'text', 'legend', 'initial_info', 'additional_text', 'country', 'city_municipality']\n",
    "\n",
    "text_features = ['name', 'commentary', 'text', 'legend', 'initial_info', 'additional_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    data[col] = data[col].fillna('nan')\n",
    "    instance_sum = 0\n",
    "    val_counts = data[col].value_counts()\n",
    "    values_to_group = []\n",
    "    for idx, name in enumerate(val_counts.index):\n",
    "        frequency = val_counts[idx]\n",
    "        if instance_sum > threshold_sum or frequency < min_freq:\n",
    "            values_to_group.append(name)\n",
    "\n",
    "        instance_sum += frequency\n",
    "    data[col] = data[col].apply(lambda x: 'uncommon' if (x in values_to_group) else x)\n",
    "\n",
    "# one hot encoding\n",
    "data = pd.get_dummies(data, columns=cols)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete unneeded features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.drop(columns=['full_nr','country_and_unit','parameter','unit','value'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## continous numeric features (nan -> 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(np.nan, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rename for xgboost (cant deal with <>[] in feature names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    if '>' in i:\n",
    "        data.rename(columns={i:i.replace('>','')}, inplace=True)\n",
    "    if '<' in i:\n",
    "        data.rename(columns={i:i.replace('<','')}, inplace=True)\n",
    "    if ']' in i:\n",
    "        data.rename(columns={i:i.replace(']','')}, inplace=True)\n",
    "    if '[' in i:\n",
    "        data.rename(columns={i:i.replace('[','')}, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resplit test/train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/prepared_ready/prep_est.csv')\n",
    "\n",
    "train = data.loc[data['source']=='train'].drop('source',axis=1)\n",
    "train, val = train_test_split(train, test_size=0.3, random_state=0)\n",
    "test = data.loc[data['source']=='test'].drop('source',axis=1)\n",
    "\n",
    "train.to_csv(f'data/prepared_ready/train_{lang}_prepared.csv')\n",
    "val.to_csv(f'data/prepared_ready/val_{lang}_prepared.csv')\n",
    "test.to_csv(f'data/prepared_ready/test_{lang}_prepared.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
