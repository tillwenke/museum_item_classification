{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/till/projects/uni/data science/museum_item_classification/setup_general.py:92: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_intermediate_ready = pd.read_csv('./data/general/combined_intermediate_ready.csv', index_col='id', dtype={'type': str})\n"
     ]
    }
   ],
   "source": [
    "from setup_general import *\n",
    "lang = 'est'\n",
    "if lang == 'en':\n",
    "    data = combined_data_fully_translated.copy()\n",
    "if lang == 'est':\n",
    "    data = combined_data.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature specific engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## units - sizes -values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish unit translation/ unification &  values to float\n",
    "data['value'] = data['value'].apply(lambda x: float(x.replace(',', '.')) if type(x) == str else x)\n",
    "\n",
    "# unify units\n",
    "data['unit'] = data['unit'].replace('10 x 15 cm','100 x 150 mm')\n",
    "\n",
    "# mm to cm\n",
    "data['value'] = data.apply(lambda item: item['value'] / 10 if item['unit'] == 'mm' else item['value'], axis=1)\n",
    "data['unit'] = data['unit'].replace('mm','cm')\n",
    "data['value'] = pd.to_numeric(data['value'])\n",
    "\n",
    "# Combine parameter, unit & w/h values to value\n",
    "def get_squared(item):\n",
    "    if ' x ' in item:\n",
    "        return item + '²'\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "def extract_width_height_from_unit_to_value(item):\n",
    "    unit = item[0]\n",
    "    value = item[1]\n",
    "    if ' x ' in unit:\n",
    "        split = unit.split(' ')\n",
    "        x = split[0]\n",
    "        y = split[2]\n",
    "        real_unit = split[3]\n",
    "        real_value = [x,y]        \n",
    "        return [real_unit, real_value]\n",
    "\n",
    "    else:\n",
    "        return [unit, value]\n",
    "\n",
    "data['unit'] = data['unit'].replace(np.nan,'*')\n",
    "data['parameter'] = data['parameter'].replace(np.nan,'*')\n",
    "data['unit'] = data['unit'].apply(lambda x: get_squared(x))\n",
    "# execution order is important\n",
    "data['value'] = data.apply(lambda item: extract_width_height_from_unit_to_value(item[['unit','value']])[1], axis=1)\n",
    "data['unit'] = data.apply(lambda item: extract_width_height_from_unit_to_value(item[['unit','value']])[0], axis=1)\n",
    "data['parameter_and_unit'] = data['parameter'] + ' IN ' + data['unit']\n",
    "\n",
    "# parameter_and_units as single features with respective values\n",
    "# parameter_and_unit turned into one hot encoded features\n",
    "data= pd.get_dummies(data, columns=['parameter_and_unit'], prefix='', prefix_sep='')\n",
    "\n",
    "def extract_value(value, present):\n",
    "    if present == 1:\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "#  for all new \"parameter with unit\" columns put the value in the column where a 1 is - others are 0 and remain 0\n",
    "for column in data.columns:\n",
    "    if ' IN ' in column and '*' not in column:\n",
    "        data[column] = data.apply(lambda item: extract_value(item['value'], item[column]), axis=1)\n",
    "\n",
    "# make all size features numeric\n",
    "def extract_height_width(item):    \n",
    "    if item == 0:\n",
    "        return [0.0,0.0]\n",
    "    else:        \n",
    "        return [float(i) for i in item]\n",
    "        \n",
    "\n",
    "for column in data.columns:\n",
    "    # all the parameter with unit columns that contain arrays that are represeted as strings\n",
    "    if (' IN ' in column) and (data[column].dtype == object):\n",
    "        data[column + '_height'] = data.apply(lambda item: extract_height_width(item[column])[0], axis=1)\n",
    "        data[column + '_width'] = data.apply(lambda item: extract_height_width(item[column])[1], axis=1)\n",
    "        pd.to_numeric(data[column + '_height'])\n",
    "        pd.to_numeric(data[column + '_width'])\n",
    "        data = data.drop(column, axis=1)\n",
    "\n",
    "for column in data.columns:\n",
    "    if (' IN ' in column):\n",
    "        data[column] = data[column].replace(np.nan,0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## country_and_unit - technique (whitespace deletion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_to_nan(item):\n",
    "    if type(item) == str:\n",
    "        item = item.strip()\n",
    "        if item == '':\n",
    "            return np.nan\n",
    "        else:\n",
    "            return item\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "data['country_and_unit'] = data.apply(lambda x: empty_to_nan(x['country_and_unit']), axis=1)\n",
    "data['technique'] = data['technique'].apply(lambda x: x.strip() if (type(x) == str) else x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## country_unit - material - technique - location (splitting for features including multiple information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_unit\n",
    "def extract_city_country(item):      \n",
    "    if (type(item) == str):\n",
    "        item = item.strip()\n",
    "        # there are some empty (non-nan) values\n",
    "        if (item == ''):\n",
    "            return [float('nan'), float('nan')]\n",
    "    \n",
    "        item = re.sub(' +', ' ', item) # remove multiple spaces\n",
    "\n",
    "        if (' ' in item) and ('Eesti' in item):        \n",
    "            split = item.split(' ')\n",
    "            return [' '.join(split[1:]), split[0]]\n",
    "        else: \n",
    "            return [float('nan'), item]\n",
    "    else:\n",
    "        return [float('nan'), float('nan')]\n",
    "\n",
    "data['city_municipality'] = data.apply(lambda item: extract_city_country(item['country_and_unit'])[0], axis=1)\n",
    "data['country'] = data.apply(lambda item: extract_city_country(item['country_and_unit'])[1], axis=1)\n",
    "\n",
    "# material\n",
    "# to make the following work even for nan values\n",
    "data['material'] = data['material'].replace(np.nan, 'nan')\n",
    "# prepare single values to be distinguishable\n",
    "data['material'] = data['material'].apply(lambda x: x.split('>'))\n",
    "\n",
    "# https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "data = data.join(pd.DataFrame(mlb.fit_transform(data.pop('material')),\n",
    "                          columns='material_' + mlb.classes_,\n",
    "                          index=data.index))\n",
    "\n",
    "# technique\n",
    "# to make the following work even for nan values\n",
    "data['technique'] = data['technique'].replace(np.nan, 'nan')\n",
    "\n",
    "# prepare single values to be distinguishable\n",
    "data['technique'] = data['technique'].apply(lambda x: x.split('>'))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "data = data.join(pd.DataFrame(mlb.fit_transform(data.pop('technique')),\n",
    "                          columns='technique_' + mlb.classes_,\n",
    "                          index=data.index), rsuffix='')\n",
    "\n",
    "# location\n",
    "data['location_city'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('linn ' in x) else 0)\n",
    "data['location_building'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('hoone ' in x) else 0)\n",
    "data['location_street'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('tänav ' in x) else 0)\n",
    "data['location_country'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('riik ' in x) else 0)\n",
    "data['location_address'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('aadress ' in x) else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start - end (formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36092/1818565639.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['startYear'].iloc[i] = data['endYear'].iloc[i]\n"
     ]
    }
   ],
   "source": [
    "#groups the year into its own column with the complete number, if NaN, then 0\n",
    "def year_Grouping(x):\n",
    "    xStr = str(x)\n",
    "    if xStr == 'nan':\n",
    "        return 0\n",
    "    if '.' in xStr:\n",
    "        xStr = xStr.split('.')\n",
    "        xStr = xStr[xStr.__len__()-1]\n",
    "        if xStr.__len__() == 2:\n",
    "            xStr = '19' + xStr\n",
    "        elif xStr == '':\n",
    "            return 0\n",
    "        return int(xStr)\n",
    "    else:\n",
    "        return int(xStr)\n",
    "\n",
    "# returns 1 if a month is given, 0 if not\n",
    "def month_Grouping(x):\n",
    "    xStr = str(x)\n",
    "    if '.' in xStr:\n",
    "        xStr = xStr.split('.')\n",
    "        if xStr[0].__len__() <= 2 and xStr[1].__len__() <= 2:\n",
    "            if xStr[1] == '':\n",
    "                return 0\n",
    "            elif xStr[1].__len__() == 1:\n",
    "                return 1\n",
    "            return 1\n",
    "        xStr = xStr[0]\n",
    "        if xStr == 'jaan':\n",
    "            return 1\n",
    "        elif xStr == 'veebr':\n",
    "            return 1\n",
    "        if xStr == 'märts':\n",
    "            return 1\n",
    "        elif xStr == 'apr':\n",
    "            return 1\n",
    "        elif xStr == 'mai':\n",
    "            return 1\n",
    "        elif xStr == 'juuni':\n",
    "            return 1\n",
    "        elif xStr == 'juuli':\n",
    "            return 1\n",
    "        elif xStr == 'aug':\n",
    "            return 1\n",
    "        elif xStr == 'sept':\n",
    "            return 1\n",
    "        elif xStr == 'okt':\n",
    "            return 1\n",
    "        elif xStr == 'nov':\n",
    "            return 1\n",
    "        elif xStr == 'dets':\n",
    "            return 1\n",
    "        elif xStr.__len__() == 1:\n",
    "            return 1\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#returns one if a day is given, 0 if not\n",
    "def day_Grouping(x):\n",
    "    xStr = str(x)\n",
    "    if '.' in xStr:\n",
    "        xStr = xStr.split('.')\n",
    "        if xStr[0].__len__() <= 2 and xStr[1].__len__() <= 2:\n",
    "            return 1\n",
    "        return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def extract_year_from_name(row):\n",
    "    name = row['name']\n",
    "    start = row['start']\n",
    "    if pd.isnull(start) and not pd.isnull(name):\n",
    "        match = re.search('\\d\\d\\d\\d', name)\n",
    "        if match:\n",
    "            start = match.group()\n",
    "    return start\n",
    "\n",
    "data['start'] = data[['name', 'start']].apply(extract_year_from_name, axis=1)    \n",
    "    \n",
    "#grouping applied to the dataframe\n",
    "data['startYear'] = data['start'].apply(year_Grouping)\n",
    "data['startMonth'] = data['start'].apply(month_Grouping)\n",
    "data['startDay'] = data['start'].apply(day_Grouping)\n",
    "        \n",
    "data['endYear'] = data['end'].apply(year_Grouping)\n",
    "data['endMonth'] = data['end'].apply(month_Grouping)\n",
    "data['endDay'] = data['end'].apply(day_Grouping)\n",
    "\n",
    "#if there is no start year, but an end year, then the start year is set to the end year\n",
    "for i in range(1,len(data)):\n",
    "    if data['startYear'].iloc[i] == 0 and data['startDay'].iloc[i] != 0:\n",
    "        data['startYear'].iloc[i] = data['endYear'].iloc[i]\n",
    "\n",
    "\n",
    "#original columns are dropped as they are no longer needed\n",
    "data.drop(['start', 'end'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## event_type (brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_brackets(x):\n",
    "    x = str(x)\n",
    "    x = x.strip('< >')\n",
    "    if '\\u200b' in x:\n",
    "        x = x.replace('\\u200b', '')\n",
    "    return x\n",
    "\n",
    "data['event_type'] = data['event_type'].apply(strip_brackets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## color (grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa6bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping colours by their base colour - to avoid too many extra cloumns when hot encoding -> could always reverse this step\n",
    "#by using  something like data['color'] = combined_data_translated['color'] ?\n",
    "\n",
    "#The base colours: red, blue, green, grey, yellow, patterned, orange, brown, white, black , pink\n",
    "#The most common/distingtive stay unchanged\n",
    "\n",
    "\n",
    "def colour_grouping(x):\n",
    "    if x in ['madara red', 'dark red', 'purple red', 'Red']:\n",
    "        return 'red'\n",
    "    elif x in ['light blue', 'dark blue', 'purple blue', 'greenish blue', 'greyish blue']:\n",
    "        return 'blue'\n",
    "    elif x in ['light green', 'light olive green', 'grey-green', 'olive green', 'greyish-olive green', 'dark green']:\n",
    "        return 'green'\n",
    "    elif x in ['bluish grey', 'dark grey', 'pinkish gray']:\n",
    "        return 'grey'\n",
    "    elif x in ['pale yellow', 'light yellow', 'orange-yellow', 'brilliant yellow']:\n",
    "        return 'yellow'\n",
    "    elif x in ['brownish orange']:\n",
    "        return 'orange'\n",
    "    elif x in ['light brown', 'dark brown', 'greyish brown', 'reddish brown', 'olive brown', 'yellowish brown']:\n",
    "        return 'brown'\n",
    "    elif x in ['yellowish white', 'bluish white']:\n",
    "        return 'white'\n",
    "    elif x in ['brownish black']:\n",
    "        return 'black'\n",
    "    elif x in ['mauve pink']:\n",
    "        return 'pink'\n",
    "    elif x in ['<patterned>', 'striped', 'checkered']:\n",
    "        return 'patterned'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "#apply colour_grouping to the dataset\n",
    "data['color'] = data['color'].apply(colour_grouping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rescaling numerics values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric features\n",
    "numeric_features = ['ks', 'musealia_seria_nr', 'musealia_queue_nr', 'collection_queue_nr', 'element_count']\n",
    "# continous numeric features (nan -> 0)\n",
    "\n",
    "data[numeric_features] = data[numeric_features].replace(np.nan, 0)\n",
    "\n",
    "data[numeric_features] = MinMaxScaler().fit_transform(data[numeric_features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## technique - material - sizes (threshold previously encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 columns found that start with technique_\n",
      "111 columns found that start with material_\n"
     ]
    }
   ],
   "source": [
    "# best found combination (local optimum on 500 estimators)\n",
    "perc = 0.98\n",
    "threshold_sum = len(data) * perc\n",
    "min_freq = 7\n",
    "\n",
    "tech = helpers.col_collection(data, 'technique_')\n",
    "mat = helpers.col_collection(data, 'material_')\n",
    "size = data.columns[data.columns.str.contains('IN')]\n",
    "\n",
    "features = [tech,mat,size]\n",
    "\n",
    "for feat in features:\n",
    "    frequencies = {}\n",
    "    for col in feat:\n",
    "        frequencies[col] = data[col].sum()\n",
    "    frequencies = dict(sorted(frequencies.items(), key=lambda item: item[1], reverse=True))\n",
    "    instance_sum = 0\n",
    "    for col in frequencies:\n",
    "        frequency = frequencies[col]\n",
    "        #if instance_sum > threshold_sum or frequency < min_freq:\n",
    "        if frequency < min_freq:\n",
    "            data.drop(columns=[col], inplace=True)\n",
    "        instance_sum += frequency\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd4a3dba",
   "metadata": {},
   "source": [
    "## hot encoding & thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical columns\n",
    "# already encoded\n",
    "# material, technique, unit, size, value\n",
    "\n",
    "cols = ['musealia_additional_nr', 'collection_mark', 'musealia_mark', 'museum_abbr', 'before_Christ', 'is_original', 'class', 'parish', 'state',  'event_type', 'participants_role', 'parish', 'color', 'collection_additional_nr', 'damages', 'participant', 'location', 'name', 'commentary', 'text', 'legend', 'initial_info', 'additional_text', 'country', 'city_municipality']\n",
    "\n",
    "text_features = ['name', 'commentary', 'text', 'legend', 'initial_info', 'additional_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    data[col] = data[col].fillna('nan')\n",
    "    instance_sum = 0\n",
    "    val_counts = data[col].value_counts()\n",
    "    values_to_group = []\n",
    "    for idx, name in enumerate(val_counts.index):\n",
    "        frequency = val_counts[idx]\n",
    "        if instance_sum > threshold_sum or frequency < min_freq:\n",
    "            values_to_group.append(name)\n",
    "\n",
    "        instance_sum += frequency\n",
    "    data[col] = data[col].apply(lambda x: 'uncommon' if (x in values_to_group) else x)\n",
    "\n",
    "# one hot encoding\n",
    "data = pd.get_dummies(data, columns=cols)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete unneeded features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.drop(columns=['full_nr','country_and_unit','parameter','unit','value'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## continous numeric features (nan -> 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(np.nan, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rename for xgboost (cant deal with <>[] in feature names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    if '>' in i:\n",
    "        data.rename(columns={i:i.replace('>','')}, inplace=True)\n",
    "    if '<' in i:\n",
    "        data.rename(columns={i:i.replace('<','')}, inplace=True)\n",
    "    if ']' in i:\n",
    "        data.rename(columns={i:i.replace(']','')}, inplace=True)\n",
    "    if '[' in i:\n",
    "        data.rename(columns={i:i.replace('[','')}, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resplit test/train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.to_csv('data/prepared_ready/prep_est.csv')\n",
    "\n",
    "train = data.loc[data['source']=='train'].drop('source',axis=1)\n",
    "train, val = train_test_split(train, test_size=0.3, random_state=0)\n",
    "test = data.loc[data['source']=='test'].drop('source',axis=1)\n",
    "\n",
    "train.to_csv(f'data/prepared_ready/train_{lang}_prepared.csv')\n",
    "val.to_csv(f'data/prepared_ready/val_{lang}_prepared.csv')\n",
    "test.to_csv(f'data/prepared_ready/test_{lang}_prepared.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
