{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_general import *\n",
    "from setup_embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_indicators = {}\n",
    "with open('data/type_indicators/type_ind_cut.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        a = line.split('\\'')\n",
    "        type = a[1]\n",
    "        indicators = a[2].split()\n",
    "        type_indicators[type] = indicators\n",
    "save_indicators = {}\n",
    "with open('data/type_indicators/save_indicator.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        a = line.split('\\'')\n",
    "        type = a[1]\n",
    "        indicators = a[2].split()\n",
    "        save_indicators[type] = indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive functions for type from text keywords\n",
    "\n",
    "def filtering(text):\n",
    "    pred = []\n",
    "    for type in types:            \n",
    "        if type in text:\n",
    "            pred.append(type)\n",
    "    if ('drawing' in text) or ('sketch' in text) or ('design' in text):\n",
    "        pred.append('design/drawing/sketch')\n",
    "    if len(pred) > 0:\n",
    "        return pred[-1]\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def indicating(text):\n",
    "    pred = []\n",
    "    for type in types:\n",
    "        for indicator in type_indicators[type]:\n",
    "            if indicator in text:\n",
    "                pred.append(type)\n",
    "    if len(pred) > 0:\n",
    "        return pred[-1]\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def save_indicating(text):\n",
    "    pred = []\n",
    "    for type in types:\n",
    "        if type in save_indicators.keys():\n",
    "            for indicator in save_indicators[type]:\n",
    "                if indicator in text:\n",
    "                    pred.append(type)\n",
    "    if len(pred) > 0:\n",
    "        return pred[-1]\n",
    "    else:\n",
    "        return -1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine models via class-probability combination (soft-voting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the full ds used for submission?\n",
    "full = True\n",
    "# submit to \n",
    "sub_name = 'kaspar_type_checks_xgrfnn_no_emb.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/till/.local/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "#define models to be used for testing use 03 for submission use full\n",
    "import pickle\n",
    "xgb = XGBClassifier()\n",
    "xgb.load_model('models/xg/full_smote100.json')\n",
    "\n",
    "rf = pickle.load(open('./models/rf/train_prep_full_best' , 'rb'))\n",
    "\n",
    "boost_emb = XGBClassifier()\n",
    "boost_emb.load_model('models/nlp/bal_full_xg_emb.json')\n",
    "\n",
    "\"\"\"\"\n",
    "nn = TabNetClassifier()\n",
    "nn.load_model('models/nn/tabnet_full.zip')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_prep.copy() if full else train_prep.copy()\n",
    "\n",
    "features = data.drop('type', axis=1)\n",
    "labels = data.type\n",
    "\n",
    "# at least xgboost cannot deal with string labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(labels)\n",
    "labels = label_encoder.transform(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=0)\n",
    "if full: X_test = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results['id'] = X_test.index\n",
    "results.set_index('id', inplace=True)\n",
    "if not full: results['type'] = y_test\n",
    "\n",
    "#results['rf'] = rf.predict(X_test)\n",
    "#results['xg'] = xgb.predict_proba(X_test)\n",
    "#results['nn'] = nn.predict(X_test.values)\n",
    "\n",
    "results['filter'] = [-1] * len(results)\n",
    "results['indi'] = [-1] * len(results)\n",
    "results['save'] = [-1] * len(results)\n",
    "results['emb'] = [[-1]] * len(results)\n",
    "\n",
    "\n",
    "results['xg'] = [[-1]] * len(results)\n",
    "results['rf'] = [[-1]] * len(results)\n",
    "results['nn'] = [[-1]] * len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9834/1971795648.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results['xg'].iloc[i] = np.array(item)\n",
      "/tmp/ipykernel_9834/1971795648.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results['rf'].iloc[i] = np.array(item)\n",
      "/tmp/ipykernel_9834/1971795648.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results['nn'].iloc[i] = np.array(item)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(xgb.predict_proba(X_test)):\n",
    "    results['xg'].iloc[i] = np.array(item)\n",
    "\n",
    "for i,item in enumerate(rf.predict_proba(X_test)):\n",
    "    results['rf'].iloc[i] = np.array(item)\n",
    "\n",
    "for i,item in enumerate(nn.predict_proba(X_test.values)):\n",
    "    results['nn'].iloc[i] = np.array(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test_curie.copy() if full else train_curie.copy()\n",
    "\n",
    "features = text.drop('type', axis=1)\n",
    "labels = text.type\n",
    "\n",
    "#text['pred'] = boost_emb.predict_proba(features)\n",
    "text['pred'] = [[-1]] * len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9834/2524059003.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['pred'].iloc[i] = np.array(item)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(boost_emb.predict_proba(features)):\n",
    "    text['pred'].iloc[i] = np.array(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# establish our own rules to determine type from text - eventually not beneficial\\ntext['filter'] = text.text_features.apply(filtering) # check for the type in the text\\ntext['indicating'] = text.text_features.apply(indicating) # check for other often occuring type indicating words\\ntext['save'] = text.text_features.apply(save_indicating) # only check for words that (almost) only occur with a certain type\\n\\ntext['filter'] = text['filter'].apply(lambda x: type_lookup[type_lookup.english == x].index[0] if x != -1 else -1)\\ntext['indicating'] = text['indicating'].apply(lambda x: type_lookup[type_lookup.english == x].index[0] if x != -1 else -1)\\ntext['save'] = text['save'].apply(lambda x: type_lookup[type_lookup.english == x].index[0] if x != -1 else -1)\\n\\nfor index, item in text.iterrows():\\n        if index in results.index:\\n            results.at[index, 'filter'] = item['filter']\\n            results.at[index, 'indi'] = item['indicating']\\n            results.at[index, 'save'] = item['save']\\n            results.at[index, 'emb'] = item['pred']\\n\""
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# establish our own rules to determine type from text - eventually not beneficial\n",
    "text['filter'] = text.text_features.apply(filtering) # check for the type in the text\n",
    "text['indicating'] = text.text_features.apply(indicating) # check for other often occuring type indicating words\n",
    "text['save'] = text.text_features.apply(save_indicating) # only check for words that (almost) only occur with a certain type\n",
    "\n",
    "text['filter'] = text['filter'].apply(lambda x: type_lookup[type_lookup.english == x].index[0] if x != -1 else -1)\n",
    "text['indicating'] = text['indicating'].apply(lambda x: type_lookup[type_lookup.english == x].index[0] if x != -1 else -1)\n",
    "text['save'] = text['save'].apply(lambda x: type_lookup[type_lookup.english == x].index[0] if x != -1 else -1)\n",
    "\n",
    "for index, item in text.iterrows():\n",
    "        if index in results.index:\n",
    "            results.at[index, 'filter'] = item['filter']\n",
    "            results.at[index, 'indi'] = item['indicating']\n",
    "            results.at[index, 'save'] = item['save']\n",
    "            results.at[index, 'emb'] = item['pred']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in text.iterrows():\n",
    "        if index in results.index:\n",
    "            results.at[index, 'emb'] = item['pred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evalaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "def vote(preds):\n",
    "    if preds[-1][0] == -1:\n",
    "        preds = preds[:-1]\n",
    "    res = np.sum(preds, axis=0)\n",
    "    return np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['prediction'] = results.apply(lambda row: vote([row.xg, row.rf, row.nn]), axis=1)\n",
    "if not full:\n",
    "    print(accuracy_score(results.type, results.prediction))\n",
    "    print(classification_report(results.type, results.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 == np.array([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to numeric\n",
    "def replace_value(value: str):\n",
    "    if pd.isnull(value):\n",
    "        return value\n",
    "    return np.float64(value.replace(',', '.'))\n",
    "\n",
    "\n",
    "# convert to numeric and only keep year part\n",
    "def replace_start_end(value: str):\n",
    "    if pd.isnull(value):\n",
    "        return value\n",
    "    if re.match('^d?ddd$', value):\n",
    "        return int(value)\n",
    "    if re.match('dddd$', value):\n",
    "        return int(value[-4:])\n",
    "    elif not value[0].isdigit():\n",
    "        return int(f'19{value[-2:]}')\n",
    "    else:\n",
    "        return nan\n",
    "\n",
    "\n",
    "def extract_year_from_name(row):\n",
    "    name = row['name']\n",
    "    start = row['start']\n",
    "    if pd.isnull(start) and not pd.isnull(name):\n",
    "        match = re.search('\\d\\d\\d\\d', name)\n",
    "        if match:\n",
    "            start = match.group()\n",
    "    return start\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df, submission=False):\n",
    "    categorical_cols = ['material', 'location', 'before_Christ', 'country_and_unit', 'technique', 'parameter',\n",
    "                        'museum_abbr', 'damages', 'state', 'color', 'event_type', 'collection_mark']\n",
    "    categorical_cols += ['unit', 'participants_role', 'participant', 'musealia_mark']\n",
    "\n",
    "    # just keeping track what values are used\n",
    "    numeric_cols = ['start', 'end', 'value', 'collection_queue_nr', 'is_original', 'ks', 'element_count',\n",
    "                    'musealia_seria_nr', 'musealia_queue_nr']\n",
    "\n",
    "    dropped_cols = ['id', 'parish']  # can't use\n",
    "    dropped_cols += ['full_nr', 'class', 'collection_additional_nr', 'additional_text', 'text', 'initial_info',\n",
    "                     'musealia_additional_nr']  # 'commentary','name', 'legend'\n",
    "\n",
    "    if not submission: dropped_cols.append('type')\n",
    "\n",
    "    df['start'] = df['start'].apply(replace_start_end)\n",
    "    df['end'] = df['end'].apply(replace_start_end)\n",
    "    df['value'] = df['value'].apply(replace_value)\n",
    "    df['start'] = df[['name', 'start']].apply(extract_year_from_name, axis=1)\n",
    "\n",
    "    df = df.drop(columns=dropped_cols)\n",
    "    df = pd.get_dummies(df, columns=categorical_cols)\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_label_from_comment(row):\n",
    "    # comment #################################################\n",
    "    comment = row['commentary']\n",
    "\n",
    "    if not pd.isnull(comment):\n",
    "        comment = str(comment).lower()\n",
    "\n",
    "        comment_dict = {\n",
    "            'lakk': 'pitser/templijäljend',\n",
    "            'must-valge negatiiv': 'fotonegatiiv',\n",
    "            'pitserilakk': 'pitser/templijäljend',\n",
    "            'käepide': 'pitsat',\n",
    "            'перф': 'fotonegatiiv',\n",
    "            'fotoemulsioon': 'fotomaterjal',\n",
    "            'plakat':'plakat'\n",
    "        }\n",
    "        for key, val in comment_dict.items():\n",
    "            if comment.startswith(key):\n",
    "                return val\n",
    "\n",
    "        if re.match('^\\d,\\d\\d\\sg$', comment):\n",
    "            return 'münt'\n",
    "\n",
    "        if 'diapositiiv' in comment:\n",
    "            return 'diapositiiv'\n",
    "\n",
    "    # name #################################################\n",
    "    name = row['name']\n",
    "\n",
    "    if not pd.isnull(name):\n",
    "        name = str(name).lower()\n",
    "        if name == ['denaar', 'killing', 'penn', 'schilling', '1/2 örtug', 'dirhem', 'fyrk']:\n",
    "            return 'münt'\n",
    "\n",
    "        for val in ['medal', 'plakat', 'märkmed', 'maal', 'kiri', 'kleit', 'kava', 'joonistus', 'graafika', 'dokument',\n",
    "                    'ajakiri', 'telegramm', 'skulptuur', 'raamat', 'postkaart', 'nukk', 'skulptuur', 'käsikiri']:\n",
    "            if name.startswith(val):\n",
    "                return val\n",
    "\n",
    "        name_dict = {\n",
    "            'kaustik': 'kaustik/vihik',\n",
    "            'vihik': 'kaustik/vihik',\n",
    "            'reprofoto': 'diapositiiv',\n",
    "        }\n",
    "        for key, val in name_dict.items():\n",
    "            if name.startswith(key):\n",
    "                return val\n",
    "    return nan\n",
    "\n",
    "def replace_predictions(labels, pred):\n",
    "    result = np.array(pred, copy=True)\n",
    "    for i, label in enumerate(labels):\n",
    "        if not pd.isnull(label) and label != 0:\n",
    "            result[i] = label\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foto diapositiiv\n",
      "paber kaustik/vihik\n",
      "foto postkaart\n",
      "pitsat pitser/templijäljend\n",
      "kiri postkaart\n",
      "kiri käsikiri\n",
      "fotonegatiiv maal\n",
      "kiri postkaart\n",
      "dokument kava\n",
      "kiri postkaart\n",
      "kava kiri\n",
      "postkaart kiri\n",
      "kiri plakat\n",
      "telegramm kiri\n",
      "foto postkaart\n",
      "paber raamat\n",
      "foto, postkaart postkaart\n",
      "käsikiri kiri\n",
      "kava dokument\n",
      "dokument kiri\n",
      "foto dokument\n",
      "kiri dokument\n",
      "silt/märk pitsat\n",
      "aukiri/auaadress kiri\n",
      "foto kiri\n",
      "postkaart telegramm\n",
      "käsikiri, muusikateos käsikiri\n",
      "käsikiri telegramm\n",
      "käsikiri märkmed\n",
      "aukiri/auaadress kiri\n",
      "dokument kiri\n",
      "kiri postkaart\n",
      "dokument kiri\n",
      "graafika joonistus\n",
      "foto kava\n",
      "kava kiri\n",
      "kiri kava\n",
      "foto kava\n",
      "käsikiri kiri\n",
      "postkaart kiri\n",
      "kiri telegramm\n",
      "ajaleht dokument\n",
      "foto postkaart\n",
      "märkmed dokument\n",
      "kiri postkaart\n",
      "käsikiri kiri\n",
      "foto diapositiiv\n",
      "kava dokument\n",
      "kavand/joonis/eskiis plakat\n",
      "kiri postkaart\n",
      "kiri postkaart\n",
      "kiri postkaart\n",
      "foto, postkaart postkaart\n",
      "kava dokument\n",
      "dokument kiri\n",
      "kiri, postkaart kiri\n",
      "dokument kaustik/vihik\n",
      "dokument märkmed\n",
      "kiri postkaart\n",
      "silt/märk nukk\n",
      "album dokument\n",
      "kava käsikiri\n",
      "postkaart kiri\n",
      "tunnistus dokument\n",
      "postkaart kiri\n",
      "postkaart kiri\n",
      "dokument kiri\n",
      "postkaart kiri\n",
      "tunnistus dokument\n",
      "kiri kava\n",
      "foto fotomaterjal\n",
      "käsikiri dokument\n",
      "postkaart kiri\n",
      "dokument käsikiri\n",
      "aukiri/auaadress kiri\n",
      "kiri postkaart\n",
      "kava kiri\n",
      "graafika maal\n",
      "foto diapositiiv\n",
      "telegramm kiri\n",
      "dokument kiri\n",
      "81\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from numpy import nan\n",
    "\n",
    "if full:\n",
    "    results.prediction = results.prediction.replace(type_lookup.id.to_list(), type_lookup.estonian.to_list())\n",
    "    #results = results.assign(prediction='foto')\n",
    "\n",
    "    a = results.prediction.copy().tolist()\n",
    "\n",
    "    df_submission = pd.read_csv(\"data/general/test.csv\")\n",
    "    x2 = preprocess_dataframe(df_submission, submission=True)\n",
    "    # reorder columns + add missing columns + remove extra columns\n",
    "    x2_labels = x2.apply(extract_label_from_comment, axis=1)\n",
    "    \n",
    "\n",
    "    results.prediction = replace_predictions(x2_labels, results.prediction)\n",
    "    submission = pd.DataFrame({'id': results.index ,'type': results.prediction})\n",
    "\n",
    "    b = results.prediction.copy().tolist()\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != b[i]:\n",
    "            count += 1\n",
    "            print(a[i], b[i])\n",
    "\n",
    "    print(count)\n",
    "\n",
    "\n",
    "    #submission.groupby('type').nunique()  # predicted classes\n",
    "\n",
    "    submission.to_csv(f'submissions/{sub_name}', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
